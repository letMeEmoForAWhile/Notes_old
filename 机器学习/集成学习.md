# 一、集成学习的概念

- 在机器学习数据挖掘的工程项目中，由于各种分类器在设计的时候都有自己的优势和缺点（即每个分类器都有自己工作偏向。），使用单一决策的弱分类器并不是一个明智的选择。**集成学习就是平衡各个分类器的优缺点，使得我们的分类任务完成得更加优秀。**

# 二、集成学习的方法

​		集成学习有四种方法。

1. 基于投票思想的多数票机制的集成分类器（MajorityVoteClassifier）
2. 基于bagging思想的套袋集成技术（BaggingClassifier）
3. 基于boosting思想的自适应增强方法（Adaboost）
4. 分层模型集成框架stacking（叠加算法）

### 1、基于投票思想的多数票机制的集成分类器（MajorityVoteClassifier）

<img src="https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20210727103017635.png" alt="image-20210727103017635" style="zoom:80%;" />

**流程**：

1. 分别训练n个弱分类器
2. 对每个弱分类器输出预测结果，并投票
3. 每个样本取投票数最多的那个预测为该样本的最终分类预测

**Pipelines的使用**

The [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) is built using a list of `(key, value)` pairs, where the `key` is a string containing the name you want to give this step and `value` is an estimator object:

```
>>> from sklearn.pipeline import Pipeline
>>> from sklearn.svm import SVC
>>> from sklearn.decomposition import PCA
>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
>>> pipe = Pipeline(estimators)
>>> pipe
Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])
```

PS：估计器（estimator）是一个可以从数据中学习的任何对象；它可以是分类器，回归器，聚类器或者是一个 从原始数据提取和过滤有用特征的变换器（transformer）

```
class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)[source]
```

- **estimators：list of  (str, estimator) tuples** 一个内容为元组的列表

- **voting：{'hard', 'soft'}, default='hard'**

  hard: 使用预测的类标签结果作为投票规则。

  soft：使用预测的类概率作为投票规则。

- **weights：array-like of shape (n_classifiers,), default=None**

  投票前，对于预测标签（hard voting）或预测概率（soft voting）给出的权重（weights）。选择None时，使用平均分布。

- **n_jobs: int, default=None**

  fit时平行使用的job数。除了使用joblib.parallel_backend恢复的模型，None表示1.-1表示使用所有处理器。

- **flatten_transform: bool, default=True**

  Affects shape of transform output only when voting=’soft’ If voting=’soft’ and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).

- verbose: bool, default=False

  True时，训练结束时会打印使用的时间。

**代码实现（使用逻辑回归、决策树、K-邻近分类器）**：

```python
## 加载相关库
from sklearn.datasets import load_iris   # 加载数据
from sklearn.model_selection import train_test_split  # 切分训练集与测试集
from sklearn.preprocessing import StandardScaler  # 标准化数据，使所有数据范围大致相同
from sklearn.preprocessing import LabelEncoder   # 标签化分类变量

## 初步处理数据
iris = load_iris()
X,y = iris.data[50:,[1,2]],iris.target[50:]
le = LabelEncoder()
y = le.fit_transform(y)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.5,random_state=1,stratify=y)

## 我们使用训练集训练三种不同的分类器：逻辑回归 + 决策树 + k-近邻分类器
from sklearn.model_selection import cross_val_score   # 10折交叉验证评价模型
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline   # 管道简化工作流
clf1 = LogisticRegression(penalty='l2',C=0.001,random_state=1)
clf2 = DecisionTreeClassifier(max_depth=1,criterion='entropy',random_state=0)
clf3 = KNeighborsClassifier(n_neighbors=1,p=2,metric="minkowski")
pipe1 = Pipeline([['sc',StandardScaler()],['clf',clf1]])
pipe3 = Pipeline([['sc',StandardScaler()],['clf',clf3]])
clf_labels = ['Logistic regression','Decision tree','KNN']
print('10-folds cross validation :\n')
for clf,label in zip([pipe1,clf2,pipe3],clf_labels):
    scores = cross_val_score(estimator=clf,X=X_train,y=y_train,cv=10,scoring='roc_auc')
    print("ROC AUC: %0.2f(+/- %0.2f)[%s]"%(scores.mean(),scores.std(),label))
```

**ps**: 

- zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象（python2中返回的是list）。

- 元组（tuple），与列表类似，也是由一些列按特定顺序排列的元素组成，但它是不可变序列（因此元组也被称为不可变的列表）。在形式上，元组的所有元素都放在一对小括号“()”中，两个相邻元素间使用逗号“,”分隔。

  与列表的区别：

  1. 列表是可变序列，元组是不可变序列，元素不可修改，除非整体替换。
  2. 列表可以通过切片访问和修改元素。元组只支持切片访问，不支持切片修改。
  3. 元组比列表访问和处理速度快。如果只需要对其中的元素进行访问，而不进行任何修改，建议使用元组而不是列表。
  4. 列表不能作为字典的键，元组可以。

```
## 我们使用MajorityVoteClassifier集成：
from sklearn.ensemble import VotingClassifier
mv_clf = VotingClassifier(estimators=[('pipe1',pipe1),('clf2',clf2),('pipe3',pipe3)],voting='soft')
clf_labels += ['MajorityVoteClassifier']
all_clf = [pipe1,clf2,pipe3,mv_clf]
print('10-folds cross validation :\n')
for clf,label in zip(all_clf,clf_labels):
    scores = cross_val_score(estimator=clf,X=X_train,y=y_train,cv=10,scoring='roc_auc')
    print("ROC AUC: %0.2f(+/- %0.2f)[%s]"%(scores.mean(),scores.std(),label))
## 对比下面结果，可以得知多数投票方式的分类算法，抗差能力更强。
```



### 2、基于bagging思想的套袋集成技术（BaggingClassifier）

##### 2.1 流程图

套袋方法可以提高不稳定模型的准确度的同时**降低过拟合的程度（可降方差**）

流程:<img src="https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20210727194543550.png" alt="image-20210727194543550" style="zoom:80%;" />

##### 2.2 投票方法与套袋方法的不同

​		投票机制在训练每个分类器的时候都是用相同的全部样本，而Bagging方法则是使用全部样本的一个随机抽样，**每个分类器使用不同的样本进行训练**。其他地方和投票方法一模一样。

##### 2.3 代码实现

```
import pandas as pd
## 我们使用葡萄酒数据集进行建模(数据处理)

# 将csv读成DataFrame格式
df_wine = pd.read_csv('wine.data',header=None)

# 为DataFrame添加列名
df_wine.columns = ['Class label', 'Alcohol','Malic acid', 'Ash','Alcalinity of ash','Magnesium', 'Total phenols',
              'Flavanoids', 'Nonflavanoid phenols','Proanthocyanins','Color intensity', 'Hue','OD280/OD315 of diluted wines','Proline'] 


# df_wine = df_wine['Class label'] != 1  # drop 1 class      
df_wine = df_wine[df_wine['Class label'] != 1]  # drop 1 class      

# 标签和特征
y = df_wine['Class label'].values
X = df_wine[['Alcohol','OD280/OD315 of diluted wines']].values

from sklearn.model_selection import train_test_split  # 切分训练集与测试集
from sklearn.preprocessing import LabelEncoder   # 标签化分类变量

# y 从 2 3 转换成0 1 
le = LabelEncoder()
y = le.fit_transform(y)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1,stratify=y)
```

使用单一决策树分类：

```text
## 我们使用单一决策树分类：
tree = DecisionTreeClassifier(criterion='entropy',random_state=1,max_depth=None)   #选择决策树为基本分类器
from sklearn.metrics import accuracy_score
tree = tree.fit(X_train,y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)
tree_train = accuracy_score(y_train,y_train_pred)
tree_test = accuracy_score(y_test,y_test_pred)
print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train,tree_test))
```





###### Note：DataFrame

​		https://zhuanlan.zhihu.com/p/351744964

​		**概念**：DataFrame是一种表格型数据结构，由行（rows）和列（columns）组成，index为行索引，column为列索引。可以理解成是一种存放**Series(带有标签的一维数组)对象**，结构类似于字典的容器。

​		**特点:** Two-dimensional（二维）, size-mutable（尺寸可变）, potentially heterogeneous tabular data（潜在的异构表格型数据）。

​		**相关的库：**pandas 

​		**使用方法：**

- 将csv文件的内容读取为DataFrame格式

  ```
  import pandas as pd
  
  path='./test.csv'
  df=pd.read_csv(path)
  ```

- 直接创建DataFrame格式数据

  ```
  data={'column1':[1,1,1,],'column2':[2,2,2],'column3':[3,3,3]}
  df=pd.DataFrame(data)
  ```

- 利用数组创建DataFrame

  ```
  data = np.random.randn(3, 4)
  df = pd.DataFrame(data, columns=['column1', 'column2', 'column3', 'column4'])
  ```

- 为DataFrame创建行索引（index）

  DataFrame的默认index是从0开始，同时支持用户自己创建index

  ```
  df.set_index('column1', inplace=True)
  print(df)
  ```

- 为DataFrame添加列名

  ```python3
  df.columns=['column1','column2','column3','column4']
  ```

- 获取DataFrame的行索引

  ```text
  index=df.index
  print(index)
  # RangeIndex(start=0, stop=568, step=1)
  ```

- 获取DataFrame的列索引

  ```
  column=df.columns
  print(column)
  # Index(['column1','column2','column3','column4'], dtype='object'
  ```

- 获取指定index的内容

  ```
  data_index=df.loc['column1'] # loc通过index来取值，不能用数字
  data_index=df.iloc[0] # iloc通过行号取值，不能用行索引名，方便用于切片
  ```

- 获取指定column的内容

  ```
  # 返回的是series对象
  data_col=df['column1']
  print(data_col)
  
  # 返回一维数组,相当于返回series对象，然后使用series的属性values
  data_col=df['column1'].values
  ```

- 获取单元内容

  ```
  a=df['column1'][0] # 先列，后行
  ```

- 获取特定内容的index

  ```
  index=df[df.column1==key].index # 返回index对象列表
  index=df[df.column1==key].index.tolist() # 返回list
  ```

- DataFrame转list

  ```text
  list=df.values.tolist()
  ```

### 3、基于boosting思想的自适应增强方法（adaboost）

**与Bagging相比，Boosting思想可以降低偏差**

自动调整权重

![image-20210803161050752](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20210803161050752.png)

```
## 我们使用Adaboost集成建模：
ada = AdaBoostClassifier(base_estimator=tree,n_estimators=500,learning_rate=0.1,random_state=1)
ada = ada.fit(X_train,y_train)
y_train_pred = ada.predict(X_train)
y_test_pred = ada.predict(X_test)
ada_train = accuracy_score(y_train,y_train_pred)
ada_test = accuracy_score(y_test,y_test_pred)
print('Adaboost train/test accuracies %.3f/%.3f' % (ada_train,ada_test))
```

