# 多元线性回归问题

### 	目标

- θj=θj-学习速率乘以j(θ)对θj的导数。

  

  ## 一、梯度下降法

  ### 多元梯度下降法Ⅰ：特征缩放

- 不同的特征在一个大致相同的范围内，可以使收敛速度更快，迭代速度更快。可以将特征除以不同的值，使得它们大致在一个近似的范围内。

- 有时也会进行均值归一化，使得均值为0，新的特征值=（特征值-平均值）/特征值的范围。

  ### 多元梯度下降法Ⅱ：学习率

  两个方面：调试和选择学习率α

  问题：不能提前知道需要多少步可以收敛。

  解决方法：

  1. 利用自动的收敛测试，如果代价函数一步J(θ)一步迭代后的下降小于一个很小的值ε，就认定为收敛。

  2. 看曲线图

     PS：当J(θ)随着迭代反而升高时，通常应该使用更小的学习率α，但是α过小时，会引起收敛速度过慢的问题。

  ### 特征和多项式回归

  不再是线性。可用二次模型、三次模型。

  ## 二、正规方程：区别于迭代方法的直接解法

  - 区别于梯度下降法：不需要迭代，只要一步就可以得到最优的θ

  - 实现方法：对所有θ的偏导为0，解出每一个θ

  ##### 矩阵实现方法

![image-20210720153656974](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20210720153656974.png)

- 设计矩阵X的构建方法:如上图所示，每一行为一个样本的各特征值。

- 不需要用特征缩放



## 三、梯度下降法和正规方程的对比

##### 	特征下降法

###### 		缺点：

- 需要选择学习率α

- 需要许多迭代

  ###### 优点：

- 即使特征数量n很大，也能很好地工作

#####    正规方程

​		优点：

- 不需要选择学习率α

- 不需要迭代

  缺点：

- 需要计算<img src="C:\Users\yyyyyyyyyyyy\AppData\Roaming\Typora\typora-user-images\image-20210726104244888.png" alt="image-20210726104244888" style="zoom: 25%;" /> ，复杂度大概为O(n^3)

- 特征数量n很大时，特别慢

  ##### Summary

- n很大时，用迭代法，例如一万以上
- 算法复杂时用迭代法