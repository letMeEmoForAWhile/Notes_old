# 零、

什么是反馈数据：

在纯摄像头视觉输入的自动驾驶系统中，训练过程中需要用到3D点云作为标签的模型，无法使用生产车捕获的反馈视频数据来改进3D对象检测模型。但是，与数据采集车的数据相比，生产车的反馈图像具有更大的规模多样性，包含更多的拐角案例，它提供的反馈数据对于提高模型的鲁棒性至关重要。

# 一、引言

## 1.1 研究背景

单目3D目标检测由于其应用简单，已成为自动驾驶的主流方法。一个突出的优点是在推理过程中不需要LiDAR点云。然而，目前的大多数方法仍然依赖于3D点云数据来标记训练阶段使用的ground truths。这种训练与推理的不一致使得大规模的反馈数据（large-scale feedback data）难以利用，增加了数据收集的费用。为了弥补这一缺陷，本文提出了一种新的弱监督单目三维目标检测方法，该方法仅用在图像上标注的二维标签来训练模型。

## 1.2 研究内容

具体来说，本文在这个任务中探索了三种类型的一致性：投影一致性、多视图一致性和方向一致性，并基于这些一致性设计了一个弱监督框架。

此外，本文提供了一种新的二维方向标注方法（2D direction labeling method）来指导模型进行准确的旋转方向预测。

## 1.3 研究结论

本文的弱监督方法与一些完全监督方法具有相当的性能。当被用作预训练方法时，本文的模型仅使用1/3的3D标签就能显著优于相应的全监督基线。

# 二、具体方法

![image-20230418112330118](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230418112330118.png)

本文提出的是一种训练方法，偏向上游工作，为了展示效果，选择具有代表性的DD3D作为我们的基本检测模型，除了训练方法之外，所有设置都不变。

- 三个输入：来自不同视角的一对图像和它们之间的相对位置关系。
- 首先，将图像对（I~1~，I~2~）送到骨干模型F中进行特征提取，生成对应的特征图f~1~，f~2~
- 然后，采用不同的头部（H~cls~是分类头部，H~3d~是目标检测头部）为f~1~、f~2~的对象生成预测。cls~i~，B~i~，i∈{1，2}
- 使用预测结果和2D标签用于计算损失。投影损失L~proj~和方向一致性损失L~rot~，多视角一致性损失L~con~
- 最终的损失由预测损失和三种一致性损失相加：$L=L_{proj}+L_{con}+L_{rot}+L_{cls}$

接下来介绍2D标签和3D预测之间的一致性和我们如何使用它们来指导模型的优化

## 2.1 投影一致性

​		为了避免在标记过程中使用3D点云注释，我们只用图像的2D ground truth。一个直观的想法就是预测的3D框可以被投影到2D图像空间，并且投影的2D框和我们的标记图像框一致。我们将此属性定义为投影一致性。

## 2.3 多视角一致性

![image-20230418171312500](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230418171312500.png)

投影一致性会将预测的3D边界框约束到相应的投影区域中，所有的3D边界框都具有相同的2D投影框，如图1（a）所示。这意味着只有投影一致性的模型将会有很多最优解，因此我们需要找到更多的约束条件来找到唯一的正确解。

如果我们从两个不同的视角观察一个物体，一个物体在两张图像中会显示不同的投影视图，但它在现实世界中的3D边界框是相同的，我们称之为多视角一致性。如图1（b）所示，两个不同的视点由两个独立的投影区域，每个投影区域都有一组受投影损失约束的最优解。但在固定对象的旋转角度时，这两个最优解集合只有一个共同的解决方案，那就是目标框。

利用这种一致性优化模型：

- 首先将来自不同的3D框转换到同一坐标系中，需要一个相对位置矩阵。两种场景：多个相机；单个相机视频序列的相邻帧。
- 将一个3D框坐标转换到另一个3D框的坐标之后，计算它们之间的差值。

## 3.3 方向一致性

如果我们在3D空间中绘制一个矢量来指示对象的方向，那么当图像投影到2D空间时，我们将在图像中获得2D矢量。我们将2D矢量称为2D方向，它与3D方向之间的关系称为方向一致性。如图a是一些示例。

<img src="https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230420151732339.png" alt="image-20230420151732339" style="zoom:67%;" />

利用==内在矩阵==，从2D方向恢复3D方向。最后利用预测的方向矢量和从2D标签转换来的方向矢量来计算损失函数。

# 三、实验

## 3.1 实验环境

使用DD3D这个网络来作为目标检测框架，保持网络原有的结构，并保持所有超参数与基线相同。

## 3.2 KITTI数据集上的实验

预测三个类别：汽车、行人、骑自行车的人，对于每个类别，有三种困难等级

表一：在KITTI验证集上对于汽车这一种类的检测结果

![image-20230420155403870](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230420155403870.png)

- 验证集上，以IOU0.5为标准，
- 该方法在“简单”、“中等”、“困难”三种等级中分别获得了54.32、42.83、40.07的鸟瞰图平均精度和49.37、39.01、36.34
- 结果表明，该方法不需要激光雷达数据，也可以实现与以前的方法相当的性能，并且在中等和较难类别中显著优于它们。作者认为这是由于远处的3D点云是稀疏的，导致之前的模型性能退化。

![image-20230420161554300](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230420161554300.png)

可视化结果

![image-20230420162210665](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230420162210665.png)

- 受益于投影一致性，2D图像中可视化的3D框与对应的目标非常吻合。
- 根据鸟瞰图的说明，我们可以看到预测边界框达到了准确的深度值和目标方向。前者得益于多视角一致性，后者是旋转损失修正了2Dground truth和3D预测框之间的方向差异。

## 3.3 消融实验

![image-20230422132422240](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230422132422240.png)

![image-20230422132435025](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230422132435025.png)

![image-20230422132457958](https://raw.githubusercontent.com/letMeEmoForAWhile/typoraImage/main/img/image-20230422132457958.png)
